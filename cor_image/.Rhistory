model3<-lm(log(Time)~.,data=id)
model3<-lm(log(Time)~.,data=ib)
summary(model3)
colnames(id$[,3:6])
colnames(id[,3:6])
colnames(ib[,3:6])
model3<-lm(log(Time)~colnames(ib[,3:6]),data=ib)
paste(colnames(ib[,3:6]), "+")
model3<-lm(log(Time)~paste(colnames(ib[,3:6]), "+"),data=ib)
model3<-lm(log(Time)~CCost +Dwgs +Length +Spans,data=ib)
summary(model3)
require(Hmisc)
n=round(sqrt(nrow(bridge)))
ib<-kNN(bridge,k=n)[,1:6] # eliminate the t/f cols
# 6.
qplot(ib$Dwgs,ib$Time)
# 7.
model<-lm(Time~Dwgs,data=ib)
summary(model)
# 8.
require(MASS)
bc<-boxcox(model)
lambda<-bc$x[which(bc$y==max(bc$y))]
ib$Time<-((ib$Time^lambda-1)/lambda)
model2<-lm(Time~Dwgs,data=ib)
summary(model2)
# model2 :
# transformed.time= 3.52128  + 0.14734 *Dwges
# 9.
model3<-lm(log(Time)~.,data=ib)
summary(model3)
plot(model3)
infulenceplot(model3)
infulencePlot(model3)
model3<-lm(log(Time)~.,data=ib)
summary(model3)
plot(model3)
influencePlot(model3)
bc2<-boxcox(model3)
lambda<-bc2$x[which(bc2$y==max(bc2$y))]
lambda
avPlot(model3)
avPlot(model3)
model3<-lm(log(Time)~.,data=ib)
summary(model3)
plot(model3)
avPlot(model3)
avPlots(model3)
vif(model3)
summary(model3)
ib$Time.n<-((ib$Time^lambda-1)/lambda)
model2<-lm(Time.n~Dwgs,data=ib)
summary(model2)
model3<-lm(log(Time)~.,data=ib)
summary(model3)
plot(model3)
influencePlot(model3)
model3<-lm(log(Time)~.,data=ib)
summary(model3)
model3<-lm(log(Time)~. -Time.n,data=ib)
summary(model3)
### Machine Learning Lab: Buliding Bridge
## 1&2 Initial Exploratory Data Analyses
setwd("~/Documents/R /Data Bootcamp/Stat course code")
bridge<-read.table("[05] Bridges.txt")
summary(bridge)
Dwgs.ct<-cut(bridge$Dwgs,breaks=c(3,5,8,max(bridge$Dwgs)),include.lowest = T)
Bin.ct<-cut(bridge$Spans,breaks=c(0,1,Inf),include.lowest = T)
# using inf here!!
## using which to break
# a[which(a %in% c(3:5))]= "3-5 drawings"
# 3.Chi-square test
#HO:They are independent
table (Dwgs.ct,Bin.ct) # (!!)
chisq.test(Dwgs.ct,Bin.ct)
# since the p-value<0.05, so we reject the null hypothesis
# and these two variables are not independent from each other.
# 4. Missing Value (need attention)
require(VIM)
require(mice)
aggr(bridge)
md.pattern(bridge)
# 5.
require(Hmisc)
n=round(sqrt(nrow(bridge)))
ib<-kNN(bridge,k=n)[,1:6] # eliminate the t/f cols
# 6.
qplot(ib$Dwgs,ib$Time)
# 7.
model<-lm(Time~Dwgs,data=ib)
summary(model)
# 8.
require(MASS)
require(car)
bc<-boxcox(model)
lambda<-bc$x[which(bc$y==max(bc$y))]
ib$Time.n<-((ib$Time^lambda-1)/lambda)
model2<-lm(Time.n~Dwgs,data=ib)
summary(model2)
# model2 :
# transformed.time= 3.52128  + 0.14734 *Dwges
# 9.
model3<-lm(log(Time)~. -Time.n,data=ib)
summary(model3)
### Machine Learning Lab: Buliding Bridge
## 1&2 Initial Exploratory Data Analyses
setwd("~/Documents/R /Data Bootcamp/Stat course code")
bridge<-read.table("[05] Bridges.txt")
summary(bridge)
Dwgs.ct<-cut(bridge$Dwgs,breaks=c(3,5,8,max(bridge$Dwgs)),include.lowest = T)
Bin.ct<-cut(bridge$Spans,breaks=c(0,1,Inf),include.lowest = T)
# using inf here!!
## using which to break
# a[which(a %in% c(3:5))]= "3-5 drawings"
# 3.Chi-square test
#HO:They are independent
table (Dwgs.ct,Bin.ct) # (!!)
chisq.test(Dwgs.ct,Bin.ct)
# since the p-value<0.05, so we reject the null hypothesis
# and these two variables are not independent from each other.
# 4. Missing Value (need attention)
require(VIM)
require(mice)
aggr(bridge)
md.pattern(bridge)
# 5.
require(Hmisc)
n=round(sqrt(nrow(bridge)))
ib<-kNN(bridge,k=n)[,1:6] # eliminate the t/f cols
# 6.
qplot(ib$Dwgs,ib$Time)
# 7.
model<-lm(Time~Dwgs,data=ib)
summary(model)
# 8.
require(MASS)
require(car)
bc<-boxcox(model)
lambda<-bc$x[which(bc$y==max(bc$y))]
ib$Time.n<-((ib$Time^lambda-1)/lambda)
model2<-lm(Time.n~Dwgs,data=ib)
summary(model2)
# model2 :
# transformed.time= 3.52128  + 0.14734 *Dwges
### Machine Learning Lab: Buliding Bridge
## 1&2 Initial Exploratory Data Analyses
setwd("~/Documents/R /Data Bootcamp/Stat course code")
bridge<-read.table("[05] Bridges.txt")
summary(bridge)
Dwgs.ct<-cut(bridge$Dwgs,breaks=c(3,5,8,max(bridge$Dwgs)),include.lowest = T)
Bin.ct<-cut(bridge$Spans,breaks=c(0,1,Inf),include.lowest = T)
# using inf here!!
## using which to break
# a[which(a %in% c(3:5))]= "3-5 drawings"
# 3.Chi-square test
#HO:They are independent
table (Dwgs.ct,Bin.ct) # (!!)
chisq.test(Dwgs.ct,Bin.ct)
# since the p-value<0.05, so we reject the null hypothesis
# and these two variables are not independent from each other.
# 4. Missing Value (need attention)
require(VIM)
require(mice)
aggr(bridge)
md.pattern(bridge)
# 5.
require(Hmisc)
n=round(sqrt(nrow(bridge)))
ib<-kNN(bridge,k=n)[,1:6] # eliminate the t/f cols
# 6.
plot(ib$Dwgs,ib$Time)
# 7.
model<-lm(Time~Dwgs,data=ib)
summary(model)
# 8.
require(MASS)
require(car)
bc<-boxcox(model)
lambda<-bc$x[which(bc$y==max(bc$y))]
ib$Time.n<-((ib$Time^lambda-1)/lambda)
model2<-lm(Time.n~Dwgs,data=ib)
summary(model2)
# model2 :
# transformed.time= 3.52128  + 0.14734 *Dwges
model3<-lm(log(Time)~. -Time.n,data=ib)
summary(model3)
plot(model3)
influencePlot(model3)
summary(ib$Time)
bc2<-boxcox(model3)
lambda<-bc2$x[which(bc2$y==max(bc2$y))]
avPlots(model3)
vif(model3)
scope=list(lower=empty.model,upper=model3)
empty.model= lm(log(Time)~1,data=ib)
full.model= model3
scope=list(lower=empty.model,upper=model3)
bothBICfull=step(empty.model,scope,direction = "Both",k=2)
bothBICfull=step(empty.model,scope,direction = "both",k=2)
bothBICempty=step(full.model,scope,direction = "both",k=2)
bothAICfull=step(empty.model,scope,direction = "both",k=2)
bothAICempty=step(full.model,scope,direction = "both",k=2)
bothBICfull=step(empty.model,scope,direction = "both",k=log(nrow(ib)))
bothBICempty=step(full.model,scope,direction = "both",k=log(nrow(ib)))
model4<-lm(log(Time)~as.factor(Spans)+Dwgs,data=ib)
summary(model4)
bothAICfull=step(empty.model,scope,direction = "both",k=2)
bothBICempty=step(full.model,scope,direction = "both",k=log(nrow(ib)))
model3.2<-lm(log(Time) ~ Dwgs + Spans,data=ib)
model4<-lm(log(Time)~as.factor(Spans)+Dwgs,data=ib)
summary(model4)
anova(model4,model3.2)
anova(model4,model3.2)
names(cars)
names(model1)
model1<-lm(dist~speed,data=cars)
model1<-lm(dist~speed,data=cars)
names(model1)
cmodel<-lm(dist~speed,data=cars)
# 2
cmodel$
coefficients()
cmodel$coefficients
cmodel$coefficients[1]
intercept=-cmodel$coefficients[1]/cmodel$coefficients[2]
slope=1/cmodel$coefficients[2]
intercept
slope
cmodel2<-lm(speed~dist,data=cars)
summary(cmodel2)
plot(cmodel2)
scatterplot(speed,dist)
scatterplot(speed,dist,data=cars)
scatterplot(cars$speed,cars$dist)
abline(cmodel2)
scatterplot.matrix(model3)
model3<-lm(log(Time)~. -Time.n,data=ib)
summary(model3)
plot(model3)
influencePlot(model3)
# 10.
bc2<-boxcox(model3)
lambda<-bc2$x[which(bc2$y==max(bc2$y))]
avPlots(model3)
vif(model3)
scatterplot.matrix(model3)
vif(model3)
scatterplot.matrix(~DArea+CCost+Dwgs+Length+Spans,data=ib)
### Machine Learning Lab: Buliding Bridge
## 1&2 Initial Exploratory Data Analyses
setwd("~/Documents/R /Data Bootcamp/Stat course code")
bridge<-read.table("[05] Bridges.txt")
summary(bridge)
Dwgs.ct<-cut(bridge$Dwgs,breaks=c(3,5,8,max(bridge$Dwgs)),include.lowest = T)
Bin.ct<-cut(bridge$Spans,breaks=c(0,1,Inf),include.lowest = T)
# using inf here!!
## using which to break
# a[which(a %in% c(3:5))]= "3-5 drawings"
# 3.Chi-square test
#HO:They are independent
table (Dwgs.ct,Bin.ct) # (!!)
chisq.test(Dwgs.ct,Bin.ct)
# since the p-value<0.05, so we reject the null hypothesis
# and these two variables are not independent from each other.
# 4. Missing Value (need attention)
require(VIM)
require(mice)
aggr(bridge)
md.pattern(bridge)
# 5.
require(Hmisc)
n=round(sqrt(nrow(bridge)))
ib<-kNN(bridge,k=n)[,1:6] # eliminate the t/f cols
# 6.
plot(ib$Dwgs,ib$Time)
# 7.
model<-lm(Time~Dwgs,data=ib)
summary(model)
# 8.
require(MASS)
require(car)
bc<-boxcox(model)
lambda<-bc$x[which(bc$y==max(bc$y))]
ib$Time.n<-((ib$Time^lambda-1)/lambda)
model2<-lm(Time.n~Dwgs,data=ib)
summary(model2)
# model2 :
# transformed.time= 3.52128  + 0.14734 *Dwges
# 9.
model3<-lm(log(Time)~. -Time.n,data=ib)
summary(model3)
plot(model3)
lambda<-bc2$x[which(bc2$y==max(bc2$y))]
avPlots(model3)
vif(model3)
scatterplot.matrix(~DArea+CCost+Dwgs+Length+Spans,data=ib)
plot(~DArea+CCost+Dwgs+Length+Spans,data=ib)
scatterplot.matrix(~DArea+CCost+Dwgs+Length+Spans,data=ib)
abline(intercept,slope)
scatterplot(cars$speed,cars$dist)
abline(intercept,slope)
abline(cmodel$coefficients[1],cmodel$coefficients[2])
plot(cars$speed,cars$dist)
abline(intercept,slope)
abline(cmodel$coefficients[1],cmodel$coefficients[2])
plot(cars$dist,cars$speed)
abline(intercept,slope)
abline(cmodel$coefficients[1],cmodel$coefficients[2])
intercept=-cmodel$coefficients[1]/cmodel$coefficients[2]
intercept
inter=cmodel$coefficients[1]
slope=cmodel$coefficients[2]
coef1=cmodel$coefficients[1]
coef2=cmodel$coefficients[2]
intercept=-coef1/coef2
slope=1/coef2
cmodel<-lm(dist~speed,data=cars)
# 2
coef1=cmodel$coefficients[1]
coef2=cmodel$coefficients[2]
intercept=-coef1/coef2
slope=1/coef2
cmodel2<-lm(speed~dist,data=cars)
# 3
plot(cars$dist,cars$speed)
abline(intercept,slope)
abline(cmodel2$coefficients[1],cmodel2$coefficients[2])
plot(cars$dist,cars$speed)
abline(intercept,slope,col=1,lty=1)
abline(cmodel2$coefficients[1],cmodel2$coefficients[2],col=2,lty=2)
plot(cars$dist,cars$speed)
abline(intercept,slope,col=1,lty=1)
abline(cmodel2$coefficients[1],cmodel2$coefficients[2],col=2,lty=2)
legend("topleft",c("calculated","regression"),col=c(1:2),lty=c(1:2))
bc2<-boxcox(model3)
lambda<-bc2$x[which(bc2$y==max(bc2$y))]
AIC(bothAICempty,bothAICfull)
summary(bothBICfull)
model4<-lm(log(Time)~as.factor(Spans)+Dwgs,data=ib)
summary(model4)
anova(model4,model3.2)
# p-value>0.05, we will choose the model using spans as factors.
AIC(model4,model3.2)
BIC(model4,model3.2)
#Retrieving the numerical measures of the iris dataset.
iris.meas = iris[, -5]
# we take out the species to make as an unsupervise learning problem
summary(iris.meas)
sapply(iris.meas, sd)
#Visualizing the width measurements.
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Scaled Iris Data")
sapply(iris.meas, sd)
#Standardizing the variables.
iris.scale = as.data.frame(scale(iris.meas))
summary(iris.scale)
sapply(iris.scale, sd)
#Visualizing the width measurements.
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Scaled Iris Data")
#Conducting the K-Means algorithm on the whole dataset.
set.seed(0)
km.iris = kmeans(iris.scale, centers = 3)
km.iris
#Visualizing the results against the truth.
par(mfrow = c(1, 2))
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Single K-Means Attempt", col = km.iris$cluster)
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "True Species", col = iris$Species)
#Plotting the cluster centers over the data.
par(mfrow = c(1, 1))
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Single K-Means Attempt", col = km.iris$cluster)
points(km.iris$centers[, 4], km.iris$centers[, 2], pch = 16, col = "blue")
km.iris$centers
require(lattice)
wireframe(Sepal.Length ~ Sepal.Width* Petal.Width, data=iris.scale)
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled")
})
points(km.iris$centers[, 1], km.iris$centers[, 2], km.iris$centers[, 4],pch = 16, col = "blue")
install.packages("scatterplot3d")
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled")
})
points(km.iris$centers[, 1], km.iris$centers[, 2], km.iris$centers[, 4],pch = 16, col = "blue")
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled",color =iris$Species)
})
main="3-D Scatterplot iris_scaled",color =Species)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled",color =Species)
})
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled",color =iris$Species)
})
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled",color =c(1:3))
})
points(km.ir
iris.scale$pcolor[km.iris$cluster==1] <- "red"
iris.scale$pcolor[km.iris$cluster==2] <- "blue"
iris.scale$pcolor[km.iris$cluster==3] <- "darkgreen"
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Width,    # z axis
main="3-D Scatterplot iris_scaled",color =pcolor)
})
iris.scale$pcolor[km.iris$cluster==1] <- "red"
iris.scale$pcolor[km.iris$cluster==2] <- "blue"
iris.scale$pcolor[km.iris$cluster==3] <- "darkgreen"
library(scatterplot3d)
with(iris.scale, {
scatterplot3d(Sepal.Length,   # x axis
Sepal.Width,     # y axis
Petal.Length,    # z axis
main="3-D Scatterplot iris_scaled",color =pcolor)
})
#Visualizing the scree plot for the scaled iris data; 3 seems like a plausible
#choice.
wssplot(iris.scale)
install.packages("rggobi")
require(rggobi")
)
require(rggobi)
library(rggobi)
g <- ggobi(iris.scale)
install.packages("rggobi")
#Visualizing the scaled data.
par(mfrow = c(1, 1))
plot(faithful.scale)
#It is important to note the non-determininstic nature of the K-Means algorithm.
#Using the Old Faithful dataset.
faithful.scale = scale(faithful)
summary(faithful.scale)
#Visualizing the scaled data.
par(mfrow = c(1, 1))
plot(faithful.scale)
#Visualizing the scaled data.
par(mfrow = c(1, 1))
plot(faithful.scale)
#Determining the number of clusters.
wssplot(faithful.scale)
## we can only find the local minimum
#Clearly, by both visual inspection and an analysis of the scree plot, a 2
#cluster solution is the most appropriate; however, let's see what happens if
#we search for a 3 cluster solution.
set.seed(0)
km.faithful1 = kmeans(faithful.scale, centers = 3) #Running the K-means procedure
km.faithful2 = kmeans(faithful.scale, centers = 3) #5 different times, but with
km.faithful3 = kmeans(faithful.scale, centers = 3) #only one convergence of the
km.faithful4 = kmeans(faithful.scale, centers = 3) #algorithm each time.
km.faithful5 = kmeans(faithful.scale, centers = 3)
#Running the algorithm 100 different times and recording the solution with the
#lowest total within-cluster variance.
set.seed(0)
km.faithfulsim = kmeans(faithful.scale, centers = 3, nstart = 100)
par(mfrow = c(2, 3))
plot(faithful, col = km.faithful1$cluster,
main = paste("Single K-Means Attempt #1\n WCV: ",
round(km.faithful1$tot.withinss, 4)))
plot(faithful, col = km.faithful2$cluster,
main = paste("Single K-Means Attempt #2\n WCV: ",
round(km.faithful2$tot.withinss, 4)))
plot(faithful, col = km.faithful3$cluster,
main = paste("Single K-Means Attempt #3\n WCV: ",
round(km.faithful3$tot.withinss, 4)))
plot(faithful, col = km.faithful4$cluster,
main = paste("Single K-Means Attempt #4\n WCV: ",
round(km.faithful4$tot.withinss, 4)))
plot(faithful, col = km.faithful5$cluster,
main = paste("Single K-Means Attempt #5\n WCV: ",
round(km.faithful5$tot.withinss, 4)))
plot(faithful, col = km.faithfulsim$cluster,
main = paste("Best K-Means Attempt out of 100\n WCV: ",
round(km.faithfulsim$tot.withinss, 4)))
kmeans(faithful.scale,centers = 3,algorithm = "Lloyd")
kmeans(faithful.scale,centers = 3,algorithm = "Forgy")
setwd("~/Documents/python/a/cor_image")
df<-read.table("comedy.csv", header = T)
df<-read.table("comedy.csv", header = F)
